<html>
<head>
<title>CSC589: Intro to Computer Vision</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: uppercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 960px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

td img {
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>CSC589 // george gu &amp; yan shi</span></h1>
</div>
</div>
<div class="container">

<h2>Final Project: Understanding Deep Convolutional Networks</h2>

<div style="float: right; padding: 20px">
<img src="images/deeplearning.png" />
<p style="font-size: 14px">The architecture of AlexNet, 
	<a href="https://papers.nips.cc/paper/4824-imagenet-
	classification-with-deep-convolutional-neural-networks.pdf">
	as shown in the original paper.</a></p>
</div>

<p>
	Deep convolutional networks can be both simple and quite convoluted. However, in our final project of the semester, we make attempts to 
	demystify what goes into a convolutional network and try to understand how it works. In order to do this, we utilized TensorFlow and AlexNet, 
	as implemented <a href="https://github.com/kratzert/finetune_alexnet_with_tensorflow">here in this repository.</a> 
</p>
<p> If you would like to see our (George and Yan's) repository, then please <a href="https://github.com/georgezgu/tune-alexnet">refer to the link here. </a>
	We forked and adapted the code from kratzert's repository based on our needs and requirements. Overall, this was a fun, quick project to do in order to 
	understand how convolutional networks function! </p>
<h3>Roles</h3>
<p>Furthermore, as far as roles are concerned, most of the number crunching and CPU running was done on George's computer. Their setup was better equipped to handle 
	the amount of data that we needed to train AlexNet on. Much of understanding AlexNet and how to use the repository to suit our needs was communicated over Facebook 
	messenger. We worked on that portion together (cloning, debugging, using git as intended in its teamwork capacity). The report will also be a mixture of both our findings 
	and understanding of the material, but Yan primarily wrote the material.</p> 

<p>Additional roles that we undertook throughout this project will be discussed throughout the report. </p>

<div style="clear:both">
<h3>Understanding Deep Convolutional Networks</h3>
<p>Deep convolutional neural networks (CNNs) are full of layers that are used to process information, specifically images in our case, to do some analysis. 
	Our architecture of the AlexNet contains eight learned layers--five convolutional and three fully-connected layers. See the above image to understand how data 
	is processed throuhgout the system. CNNs share weights in convolutional layers, which means that the same filter is used in each field and improves performance overall. 
	All of the processes should ideally happen on the GPU, as it is faster, but in some cases will need to be done on the CPU depending on the OS of the system being used 
	(in our case, our processing happened on the CPU). If the dataset is small enough, the time it takes to run on the CPU is negilible. However, bigger datasets will require more machinepower. 
	AlexNet in particular is supervised deep learning, meaning that we are providing the network a training dataset, a validation dataset, and a testing dataset (70-15-15 breakup, if possible). 
	The network takes the training dataset, processes it, and validates it against the validation set. The testing set should be images that the network has never seen before in order to properly test 
	the network. For our particular setup, we have a pretrained network that already has weights associated with it. We will now discuss finetuning AlexNet to our new datasets from the Professor (dog vs hotdog) 
	and from ImageNet.  </p>

<h3>Tuning AlexNet Pt. 1</h3>
<p> Finetuning AlexNet, thankfully, was not a challenge but did require some work in modifying the code to suit our needs, especially with the first set of images being only 58 images total. 
	The small batch required us to consider parts of the code as George attempted to run parts of it. We had to adjust the <code>learning params</code> in <code>finetune.py</code>. We adjusted our <code>batch_size</code>
	as 8 to avoid hitting division by zero errors. We also took away one of the <code>train_layers</code> that we wanted to look at and focused primarily on the last two, fc8 and fc7. We kept the rest of the code the same 
	and then made our <code>.txt</code> files to include the paths to our training and validation images, respectively. We classified dogs as 1s and hot dogs (and food related items) as 0. There were several images 
	provided to us where the dogs were also in hot dog costumes. 
</p>

<h3>Results: Hot Dog or Just Dog?</h3>
<p> Our results were tested on 9 images! Not so surprisingly, AlexNet was able to identify the images accurately for 5/9 images (3 true positives, 2 false positives), as seen below, where food is 
	labelled appropriately and dogs are labelled appropriately. The algorithm failed to notice the Corgi in the hot dog costume as well as the Dachscund in the hot dog costume. It also miscategorized the 
	very left image as a dog, when it's actually hot dogs. For the most part, though, the algorithm did a good job in identifying the images, likely because of the fact that AlexNet was already pretrained on
	images of dogs and hotdogs, but it was likely just introduced to dogs in costumes. Perhaps with a larger dataset to train AlexNet on, we could arrive to be <em>more</em> reliant results, but a success rate of
	a little over 50% is not bad for the given dataset. </p>
	<img src="images/results/hotdogordog.png" width="900"/>

<h3>Tuning AlexNet Pt. 2</h3>
<p> After finetuning AlexNet to our dog/hotdog dataset, we were required to train the neural network on another set of data. We chose the dataset called Caltech101, which is linked below in the results (if you'd like
	to learn more about the contents of the dataset). This was significantly smaller than the dataset that we were linked to (CIFAR) and, given our computational power and prowess in writing scripts that would automate the <code>.txt</code>
	file creation, it was in our favor to pick a smaller dataset. Nonetheless, Caltech 101 consists of 101 classes with approximately 50 or so images per class, at variable resolution. Some classes have more images than others, airplanes, 
	for example, have over 700. This dataset was created by Fei Fei Li. Furthermore, CIFAR was stored in a format that was unfamiliar with both of us and would require more time than we could afford in order to tackle the problem. Combined with
	the breadth of the dataset and the datatype, it was better to go with CIFAR. As the algorithm that we are using requires a <code>.txt</code> file, George wrote a script that would automate the process and would give us two files to work with in the end
	<em>with</em> the proper labels. What this entails is parsing through the files to find the filepaths and to match the images accordingly based on the parsing to have a file containing training images and validation images.</p>

<h3>Results: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">Caltech 101 Dataset</a></h3>
<p> Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, 
	quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore 
	eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>

<div style="clear:both" >
<h3>Final Thoughts</h3>
<p> For a final project, this was interesting! We were able to properly problem solve our way through understanding and using the repository that was provided to us. We were also able to effectively use git and GitHub as a means of collaboration and used Facebook 
	in order to communicate when we made changes so that we could pull said changes down from GitHub. For the future, it would be nice to dive deeper into convlutional networks and in understanding the algorithm in-depth, and perhaps that can be done through an artifiical 
	intelligence class. Nonetheless, it is quite impressive that AlexNet was able to analyze images the way that it did and at an impeccable accuracy as well.  

</p>
</body>
</html>
